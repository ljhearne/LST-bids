{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BIDS behavioural data and meta-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LST:\n",
      "APM:\n",
      "VS:\n",
      "fg tests:\n",
      "Participants.tsv:\n",
      "Dataset description:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from scipy import io\n",
    "RAW_DIR =  '/home/lukehearne/qm89_scratch/LSTBIDS/0_behaviour/behavioural_data/' # raw behav\n",
    "BIDS_DIR = '/home/lukehearne/qm89_scratch/LSTBIDS/LST_bids/' # i.e., the output dir\n",
    "SUBJ_LIST = np.arange(1,66,1)\n",
    "\n",
    "#--------#\n",
    "# LST ---#\n",
    "#--------#\n",
    "print('LST:')\n",
    "# 3 16 32 44 are no good in the LST.\n",
    "bad_subjs = np.array((3,16,32,44))\n",
    "for subj in np.delete(SUBJ_LIST,bad_subjs-1):\n",
    "    #print('\\tSubj:',subj)\n",
    "    file = 'Sub_'+str(subj)+'_fMRI_LST_results.mat'\n",
    "    data = io.loadmat(RAW_DIR+file)\n",
    "    #print('\\t',data.keys())\n",
    "\n",
    "    fMRI_info = data['fMRI_out']\n",
    "    runs = np.arange(1,4,1) #there are 3 runs in the task\n",
    "    for i in runs:\n",
    "        #print('\\t\\tRun:',i)\n",
    "        #index by run\n",
    "        rundata = fMRI_info[fMRI_info[:,0]==i,:]\n",
    "\n",
    "        # onset\n",
    "        df = pd.DataFrame(data=rundata[:,2],columns=['onset'])\n",
    "\n",
    "        # add the duration\n",
    "        df['duration'] = 5\n",
    "\n",
    "        # trial type\n",
    "        df['trial_type'] = rundata[:,1]\n",
    "\n",
    "        # put in the actual condition names\n",
    "        df['trial_type'].replace(to_replace=1, value='Binary',inplace=True)\n",
    "        df['trial_type'].replace(to_replace=2, value='Ternary',inplace=True)\n",
    "        df['trial_type'].replace(to_replace=3, value='Quaternary',inplace=True)\n",
    "        df['trial_type'].replace(to_replace=4, value='Null',inplace=True)\n",
    "\n",
    "        # add the response time\n",
    "        rt = rundata[:,6]\n",
    "        rt[rt>2] = float('NaN')\n",
    "        df['response_time'] = rt\n",
    "\n",
    "        # add the accuracy\n",
    "        df['accuracy'] = rundata[:,5]\n",
    "\n",
    "        # we also want to add motor and confidence information\n",
    "        df['motor_onset'] = rundata[:,3]\n",
    "        df['motor_duration'] = 2\n",
    "\n",
    "        df['confidence_onset'] = rundata[:,4]\n",
    "        df['confidence_duration'] = 2\n",
    "\n",
    "        #sort by the onset\n",
    "        df.sort_values('onset', inplace=True)\n",
    "\n",
    "        # print the dataframe to check our working  \n",
    "        #print(df.head(20))\n",
    "\n",
    "        # save data\n",
    "        subj_label = str(subj).zfill(2)\n",
    "        subj_path = BIDS_DIR + 'sub-' + subj_label + '/func/'\n",
    "        if not os.path.exists(subj_path):\n",
    "            os.makedirs(subj_path)\n",
    "        filename = (subj_path + 'sub-' + subj_label + '_task-LST_run-' \n",
    "                    + str(i) +'_events.tsv')\n",
    "        #print(filename)\n",
    "        df.to_csv(filename, index=False, sep='\\t',na_rep='n/a')\n",
    "        \n",
    "#--------#\n",
    "# APM ---#\n",
    "#--------#\n",
    "print('APM:')\n",
    "set1_answers = np.array((8, 4, 5, 1, 2, 5, 6, 3, 7, 8, 7, 6))\n",
    "set2_answers = np.array((5, 1, 7, 4, 3, 1, 6, 1, 8, 4, 5, 6, 2, 1,\n",
    "                         2, 4, 6, 7, 3, 8, 8, 7, 6, 3, 7, 2, 7, 5,\n",
    "                         6, 5, 4, 8, 5, 1, 3, 2))\n",
    "APM_set1_accuracy = np.zeros((len(SUBJ_LIST)))\n",
    "APM_set2_accuracy = np.zeros((len(SUBJ_LIST)))\n",
    "subj_labels = []\n",
    "for i,subj in enumerate(SUBJ_LIST):\n",
    "    #print('\\tSubj:',subj)\n",
    "    subj_labels.append('sub-'+str(subj).zfill(2))\n",
    "    #set 1\n",
    "    file = 'Sub_'+str(subj)+'_APM_Set1_results.mat'\n",
    "    data = io.loadmat(RAW_DIR+file)\n",
    "    resp = data['resp'][0]\n",
    "    APM_set1_accuracy[i] = np.sum(set1_answers==resp)\n",
    "    \n",
    "    #set 2\n",
    "    file = 'Sub_'+str(subj)+'_APM_Set2_results.mat'\n",
    "    data = io.loadmat(RAW_DIR+file)\n",
    "    resp = data['resp'][0]\n",
    "    if len(resp) > 36:\n",
    "        resp = resp[0:36]\n",
    "    APM_set2_accuracy[i] = np.sum(set2_answers==resp)\n",
    "\n",
    "# save data\n",
    "if not os.path.exists(BIDS_DIR + 'phenotype/'):\n",
    "    os.makedirs(BIDS_DIR + 'phenotype/')\n",
    "tsv_filename = BIDS_DIR + 'phenotype/APM.tsv'\n",
    "df = pd.DataFrame(index=subj_labels)\n",
    "df['APM_set1_accuracy'] = APM_set1_accuracy\n",
    "df['APM_set2_accuracy'] = APM_set2_accuracy\n",
    "df.to_csv(tsv_filename, sep='\\t', index_label='participant_id')\n",
    "\n",
    "# create json\n",
    "# meta information\n",
    "meta = {}\n",
    "mtm = {'Description': 'The Ravens Advanced Progressive Matrices'}\n",
    "mtm['TermURL'] = 'https://link.springer.com/referenceworkentry/10.1007/978-3-319-28099-8_69-1'\n",
    "meta['MeasurementToolMetadata'] = mtm\n",
    "# column information\n",
    "meta['APM_set1_accuracy'] = {}\n",
    "meta['APM_set1_accuracy']['Description'] = 'Total accuracy for set 1 of the APM'\n",
    "meta['APM_set2_accuracy'] = {}\n",
    "meta['APM_set2_accuracy']['Description'] = 'Total accuracy for set 2 of the APM'\n",
    "\n",
    "json_filename = BIDS_DIR + 'phenotype/APM.json'\n",
    "with open(json_filename, 'w') as jsonFile:\n",
    "    json.dump(meta, jsonFile, indent=4)\n",
    "\n",
    "#-------#\n",
    "# VS ---#\n",
    "#-------#  \n",
    "print('VS:')\n",
    "n_trials = 240\n",
    "n_trials_cond = np.int(n_trials/3)\n",
    "VS_acc_avg = np.zeros((len(SUBJ_LIST),3))\n",
    "VS_rt_avg = np.zeros((len(SUBJ_LIST),3))\n",
    "\n",
    "for i,subj in enumerate(SUBJ_LIST):\n",
    "    try:\n",
    "        #print('Subj:',subj)\n",
    "        file = 'Sub_'+str(subj)+'_VS_results.mat'\n",
    "        data = io.loadmat(RAW_DIR+file)\n",
    "\n",
    "        trial_order = data['param'][0][0][2][0].copy()\n",
    "        trial_order_sort = np.argsort(trial_order)\n",
    "\n",
    "        #acc\n",
    "        acc = data['rec'][0][0][2][0]\n",
    "        acc = acc[trial_order_sort]\n",
    "        acc = np.reshape(acc,(n_trials_cond,3),order='F')\n",
    "        VS_acc_avg[i,:] = np.mean(acc,axis=0)\n",
    "\n",
    "        #rt\n",
    "        rt = data['rec'][0][0][0][0].copy()\n",
    "        rt = rt[trial_order_sort]\n",
    "        rt = np.reshape(rt,(n_trials_cond,3),order='F')\n",
    "        rt[acc==0] = float('NaN')\n",
    "        VS_rt_avg[i,:] = np.nanmean(rt,axis=0)\n",
    "    except:\n",
    "        VS_acc_avg[i,:] = float('NaN')\n",
    "        VS_rt_avg[i,:] = float('NaN')\n",
    "\n",
    "# save data\n",
    "if not os.path.exists(BIDS_DIR + 'phenotype/'):\n",
    "    os.makedirs(BIDS_DIR + 'phenotype/')\n",
    "tsv_filename = BIDS_DIR + 'phenotype/VisualSearch.tsv'\n",
    "df = pd.DataFrame(index=subj_labels)\n",
    "df['VS_acc_avg_set8'] = VS_acc_avg[:,0]\n",
    "df['VS_acc_avg_set16'] = VS_acc_avg[:,1]\n",
    "df['VS_acc_avg_set24'] = VS_acc_avg[:,2]\n",
    "df['VS_rt_avg_set8'] = VS_rt_avg[:,0]\n",
    "df['VS_rt_avg_set16'] = VS_rt_avg[:,1]\n",
    "df['VS_rt_avg_set24'] = VS_rt_avg[:,2]\n",
    "df.to_csv(tsv_filename, sep='\\t', index_label='participant_id',na_rep='n/a')\n",
    "\n",
    "# create json\n",
    "# meta information\n",
    "meta = {}\n",
    "mtm = {'Description': 'Visual Search Task - 43 participants completed a conjunction visual search task in which they were instructed to report the orientation of a target letter “L” (rotated 90° leftward or rightward) among “T” distractors in set sizes of 8, 16, or 24 items (80 trials each, 240 total). The search cost was defined as the increase in reaction time between the smallest and largest set sizes. This task was chosen as a “low reasoning” counterpart to the Ravens Progressive Matrices to demonstrate the specificity of brain–behavior correlations, as described in detail in the Results.'}\n",
    "mtm['TermURL'] = 'n/a'\n",
    "meta['MeasurementToolMetadata'] = mtm\n",
    "# column information\n",
    "meta['VS_acc_avg_set8'] = {}\n",
    "meta['VS_acc_avg_set8']['Description'] = 'Average visual search accuracy for sets of 8 stimuli'\n",
    "meta['VS_acc_avg_set16'] = {}\n",
    "meta['VS_acc_avg_set16']['Description'] = 'Average visual search accuracy for sets of 16 stimuli'\n",
    "meta['VS_acc_avg_set24'] = {}\n",
    "meta['VS_acc_avg_set24']['Description'] = 'Average visual search accuracy for sets of 24 stimuli'\n",
    "meta['VS_rt_avg_set8'] = {}\n",
    "meta['VS_rt_avg_set8']['Description'] = 'Average reaction time (correct trials only) for sets of 8 stimuli'\n",
    "meta['VS_rt_avg_set16'] = {}\n",
    "meta['VS_rt_avg_set16']['Description'] = 'Average reaction time (correct trials only) for sets of 8 stimuli'\n",
    "meta['VS_rt_avg_set24'] = {}\n",
    "meta['VS_rt_avg_set24']['Description'] = 'Average reaction time (correct trials only) for sets of 8 stimuli'\n",
    "\n",
    "json_filename = BIDS_DIR + 'phenotype/VisualSearch.json'\n",
    "with open(json_filename, 'w') as jsonFile:\n",
    "    json.dump(meta, jsonFile, indent=4)\n",
    "    \n",
    "#------------------------#\n",
    "# Additional fg tests ---#\n",
    "#------------------------#\n",
    "print('fg tests:')\n",
    "df_fg = pd.read_csv(RAW_DIR + 'fluid_tests_fMRI.csv')\n",
    "\n",
    "# form boards\n",
    "tsv_filename = BIDS_DIR + 'phenotype/FormBoards.tsv'\n",
    "df = pd.DataFrame(index=subj_labels)\n",
    "df['Total_correct'] = df_fg.loc[0:64,'FB-correct'].values\n",
    "df['Total_incorrect'] = df_fg.loc[0:64,'FB-incorrect'].values\n",
    "df.to_csv(tsv_filename, sep='\\t', index_label='participant_id',na_rep='n/a')\n",
    "# create json\n",
    "# meta information\n",
    "meta = {}\n",
    "mtm = {'Description': 'The Minnesota Paper Form Board Test'}\n",
    "mtm['TermURL'] = 'https://en.wikipedia.org/wiki/Minnesota_Paper_Form_Board_Test'\n",
    "meta['MeasurementToolMetadata'] = mtm\n",
    "# column information\n",
    "meta['Total_correct'] = {}\n",
    "meta['Total_correct']['Description'] = 'Total number of correct items (total score is often correct - incorrect, participants can choose not to answer items)'\n",
    "meta['Total_incorrect'] = {}\n",
    "meta['Total_incorrect']['Description'] = 'Total number of incorrect items'\n",
    "\n",
    "json_filename = BIDS_DIR + 'phenotype/FormBoards.json'\n",
    "with open(json_filename, 'w') as jsonFile:\n",
    "    json.dump(meta, jsonFile, indent=4)\n",
    "    \n",
    "# paper-folding\n",
    "tsv_filename = BIDS_DIR + 'phenotype/PaperFolding.tsv'\n",
    "df = pd.DataFrame(index=subj_labels)\n",
    "df['Total_correct'] = df_fg.loc[0:64,'PF-correct'].values\n",
    "df['Total_incorrect'] = df_fg.loc[0:64,'PF-incorrect'].values\n",
    "df.to_csv(tsv_filename, sep='\\t', index_label='participant_id',na_rep='n/a')\n",
    "# create json\n",
    "# meta information\n",
    "meta = {}\n",
    "mtm = {'Description': 'Paper folding test of fluid intelligence'}\n",
    "mtm['TermURL'] = 'n/a'\n",
    "meta['MeasurementToolMetadata'] = mtm\n",
    "# column information\n",
    "meta['Total_correct'] = {}\n",
    "meta['Total_correct']['Description'] = 'Total number of correct items (total score is often correct - incorrect, participants can choose not to answer items)'\n",
    "meta['Total_incorrect'] = {}\n",
    "meta['Total_incorrect']['Description'] = 'Total number of incorrect items'\n",
    "\n",
    "json_filename = BIDS_DIR + 'phenotype/PaperFolding.json'\n",
    "with open(json_filename, 'w') as jsonFile:\n",
    "    json.dump(meta, jsonFile, indent=4)\n",
    "    \n",
    "#----------------------------#\n",
    "# Create participants.tsv ---#\n",
    "#----------------------------#\n",
    "print('Participants.tsv:')\n",
    "df_demo = pd.read_csv(RAW_DIR + 'Project_Summary_DEIDENTIFIED.csv')\n",
    "age = df_demo.loc[0:64,'Age'].values\n",
    "sex = df_demo.loc[0:64,'Gender (M=1, F=2)']\n",
    "sex[sex==1] = 'M'\n",
    "sex[sex==2] = 'F'\n",
    "\n",
    "# write df_out\n",
    "df = pd.DataFrame({'age': age, 'sex': sex.values}, index=subj_labels)\n",
    "df.to_csv(BIDS_DIR+'participants.tsv', sep='\\t', index_label='participant_id',na_rep='n/a')\n",
    "\n",
    "#-----------------------#\n",
    "# Extra demographics ---#\n",
    "#-----------------------#\n",
    "tsv_filename = BIDS_DIR + 'phenotype/Extended_demographics.tsv'\n",
    "df = pd.DataFrame(index=subj_labels)\n",
    "df['Years_of_education'] = df_demo.loc[0:64,'Years of Education'].values\n",
    "df['Sudoku_experience'] = df_demo.loc[0:64,'Sudoku Score (1-5)'].values\n",
    "df['Motivation_rating'] = df_demo.loc[0:64,'Motivation'].values\n",
    "df['Sleep_rating'] = df_demo.loc[0:64,'Sleep Report (0=no, 1=mild, 2=yes)'].values\n",
    "df.to_csv(tsv_filename, sep='\\t', index_label='participant_id',na_rep='n/a')\n",
    "\n",
    "# create json\n",
    "# meta information\n",
    "meta = {}\n",
    "mtm = {'Description': 'Extended demographic information'}\n",
    "mtm['TermURL'] = 'n/a'\n",
    "meta['MeasurementToolMetadata'] = mtm\n",
    "# column information\n",
    "meta['Years_of_education'] = {}\n",
    "meta['Years_of_education']['Description'] = 'Total number of education in years including counting from primary school'\n",
    "meta['Sudoku_experience'] = {}\n",
    "meta['Sudoku_experience']['Description'] = 'Participant rating of sudoku experience from 1 to 5, 1 being low (never played sudoku), to 5 (daily sudoku use)'\n",
    "meta['Motivation_rating'] = {}\n",
    "meta['Motivation_rating']['Description'] = '\"How motivated were you to complete the task?\" where 1 = not motivated at all, 5 = very motivated'\n",
    "meta['Sleep_rating'] = {}\n",
    "meta['Sleep_rating']['Description'] = 'Did the participant fall asleep in the scan? 0 = no, 1 = maybe, 2 = yes'\n",
    "\n",
    "json_filename = BIDS_DIR + 'phenotype/Extended_demographics.json'\n",
    "with open(json_filename, 'w') as jsonFile:\n",
    "    json.dump(meta, jsonFile, indent=4)\n",
    "    \n",
    "#---------------------#\n",
    "# Data description ---#\n",
    "#---------------------#\n",
    "print('Dataset description:')\n",
    "# create the dataset description - basically verbatim from the BIDS standard\n",
    "data= OrderedDict()\n",
    "#General fields, shared with MRI BIDS and MEG BIDS:\n",
    "#Required fields:\n",
    "#name of the dataset\n",
    "data['Name'] = 'LST'\n",
    "\n",
    "#The version of the BIDS standard that was used\n",
    "data['BIDSVersion'] = '1.2.0'\n",
    "\n",
    "#Recommended fields:\n",
    "#what license is this dataset distributed under? The use of license name abbreviations is suggested for specifying a license. A list of common licenses with suggested abbreviations can be found in appendix III.\n",
    "data['License'] = 'PD'\n",
    "\n",
    "#List of individuals who contributed to the creation/curation of the dataset\n",
    "data['Authors'] = ['Luke Hearne','Luca Cocchi','Jason Mattingley']\n",
    "\n",
    "#who should be acknowledged in helping to collect the data\n",
    "data['Acknowledgements'] = 'This work was supported by the Australian Research Council (ARC) Special Research Initiatives Science of Learning Research Centre (SR120300015), and the ARC Centre of Excellence for Integrative Brain Function (ARC Centre Grant CE140100007). J.B.M. was supported by an ARC Australian Laureate Fellowship (FL110100103). L.C. was supported by a National Health and Medical Research Council (NHMRC) grant (APP1099082). A.Z. was supported by an NHMRC Career Development Fellowship (GNT1047648). L.J.H. was supported by an Australian Postgraduate Award. We thank Oscar Jacoby and Zoie Nott for data-collection assistance, Tong Wu for assistance with imaging analysis, and Dr. Kieran OBrien, Associate Professor Markus Barth, and Dr. Steffen Bollmann for performing the sequence optimizations.'\n",
    "\n",
    "#Instructions how researchers using this dataset should acknowledge the original authors. This field can also be used to define a publication that should be cited in publications that use the dataset\n",
    "data['HowToAcknowledge'] = 'Please cite: Hearne, L. J., Cocchi, L., Zalesky, A., & Mattingley, J. B. (2017). Reconfiguration of brain network architectures between resting-state and complexity-dependent cognitive reasoning. Journal of Neuroscience, 37(35), 8399-8411.'\n",
    "\n",
    "#sources of funding (grant numbers)\n",
    "data['Funding'] = ['See Acknowledgements']\n",
    "\n",
    "#a list of references to publication that contain information on the dataset, or links.\n",
    "data['ReferencesAndLinks'] = ['https://doi.org/10.1523/JNEUROSCI.0485-17.2017']\n",
    "\n",
    "#the Document Object Identifier of the dataset (not the corresponding paper).\n",
    "data['DatasetDOI'] = ''\n",
    "\n",
    "dataset_json_folder = BIDS_DIR\n",
    "dataset_json_name = dataset_json_folder+'/'+'dataset_description.json'\n",
    "\n",
    "with open(dataset_json_name, 'w') as ff:\n",
    "    json.dump(data, ff,sort_keys=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "\t rest- 1 json file not found\n",
      "01\n",
      "\t rest- 2 json file not found\n",
      "02\n",
      "\t LST- 1 json file not found\n",
      "04\n",
      "\t LST- 2 json file not found\n",
      "04\n",
      "\t rest- 1 json file not found\n",
      "04\n",
      "\t rest- 2 json file not found\n",
      "06\n",
      "\t LST- 1 json file not found\n",
      "06\n",
      "\t LST- 2 json file not found\n",
      "06\n",
      "\t LST- 3 json file not found\n",
      "06\n",
      "\t rest- 1 json file not found\n",
      "07\n",
      "\t LST- 1 json file not found\n",
      "08\n",
      "\t LST- 2 json file not found\n",
      "08\n",
      "\t rest- 2 json file not found\n",
      "09\n",
      "\t LST- 1 json file not found\n",
      "09\n",
      "\t LST- 2 json file not found\n",
      "09\n",
      "\t rest- 2 json file not found\n",
      "12\n",
      "\t LST- 1 json file not found\n",
      "16\n",
      "\t LST- 1 json file not found\n",
      "16\n",
      "\t LST- 2 json file not found\n",
      "16\n",
      "\t LST- 3 json file not found\n",
      "16\n",
      "\t rest- 1 json file not found\n",
      "16\n",
      "\t rest- 2 json file not found\n",
      "19\n",
      "\t LST- 1 json file not found\n",
      "21\n",
      "\t LST- 2 json file not found\n",
      "22\n",
      "\t LST- 1 json file not found\n",
      "23\n",
      "\t LST- 1 json file not found\n",
      "24\n",
      "\t LST- 1 json file not found\n",
      "26\n",
      "\t LST- 2 json file not found\n",
      "26\n",
      "\t rest- 1 json file not found\n",
      "26\n",
      "\t rest- 2 json file not found\n",
      "27\n",
      "\t rest- 2 json file not found\n",
      "65\n",
      "\t rest- 1 json file not found\n"
     ]
    }
   ],
   "source": [
    "#append dcm2nii json files with task names\n",
    "for i,subj in enumerate(SUBJ_LIST):\n",
    "    subj_label = str(subj).zfill(2)\n",
    "    #LST\n",
    "    for run in [1,2,3]:\n",
    "        filename = BIDS_DIR + 'sub-' + subj_label + '/func/sub-' + subj_label + '_task-LST_run-' + str(run) + '_bold.json'\n",
    "        if not os.path.exists(filename):\n",
    "            print(subj_label)\n",
    "            print('\\t LST-',str(run),'json file not found')\n",
    "            continue\n",
    "        a_dict = {'TaskName': 'LST'}\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        data.update(a_dict)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(data,f)\n",
    "\n",
    "    # rest\n",
    "    for run in [1,2]:\n",
    "        filename = BIDS_DIR + 'sub-' + subj_label + '/func/sub-' + subj_label + '_task-rest_run-' + str(run) + '_bold.json'\n",
    "        if not os.path.exists(filename):\n",
    "            print(subj_label)\n",
    "            print('\\t rest-',str(run),'json file not found')\n",
    "            continue\n",
    "        a_dict = {'TaskName': 'rest'}\n",
    "        with open(filename) as f:\n",
    "            data = json.load(f)\n",
    "        data.update(a_dict)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
